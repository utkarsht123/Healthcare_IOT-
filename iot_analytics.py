# -*- coding: utf-8 -*-
"""iot analytics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10F6F7GbClaljghaP_CCOZD4ormpo0QxK
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix
from sklearn.impute import KNNImputer
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import plotly.graph_objects as go
import plotly.express as px
import dash
from dash import dcc, html
from dash.dependencies import Input as DashInput, Output
import warnings
warnings.filterwarnings('ignore')

# Set random seeds for reproducibility
np.random.seed(42)
torch.manual_seed(42)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(42)

# Check if CUDA is available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Custom dataset class for PyTorch
class TimeSeriesDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.long)

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

# Attention mechanism for PyTorch
class SelfAttention(nn.Module):
    def __init__(self, hidden_dim, num_heads=4):
        super(SelfAttention, self).__init__()
        self.multihead_attn = nn.MultiheadAttention(hidden_dim, num_heads)

    def forward(self, x):
        # x shape: [batch_size, seq_len, hidden_dim]
        # For multihead attention, we need [seq_len, batch_size, hidden_dim]
        x = x.permute(1, 0, 2)
        attn_output, attn_weights = self.multihead_attn(x, x, x)
        # Return to original shape
        return attn_output.permute(1, 0, 2), attn_weights

# Model architecture
class BiLSTMAttentionModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_layers, num_classes, dropout=0.5):
        super(BiLSTMAttentionModel, self).__init__()

        # CNN layers for spatial features
        self.conv1 = nn.Conv1d(input_dim, 64, kernel_size=3, padding=1)
        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)
        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1)
        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)

        # Bidirectional LSTM
        self.lstm = nn.LSTM(
            input_size=input_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=True,
            bidirectional=True,
            dropout=dropout if num_layers > 1 else 0
        )

        # Self-attention mechanism
        self.attention = SelfAttention(hidden_dim * 2)  # *2 for bidirectional
        self.layer_norm = nn.LayerNorm(hidden_dim * 2)

        # Calculate CNN output size
        self.cnn_output_size = self._get_cnn_output_size(30, input_dim)  # 30 is sequence length

        # Fully connected layers
        self.fc1 = nn.Linear(self.cnn_output_size + (hidden_dim * 2 * 30), 256)
        self.dropout1 = nn.Dropout(dropout)
        self.fc2 = nn.Linear(256, 128)
        self.dropout2 = nn.Dropout(0.3)
        self.fc3 = nn.Linear(128, num_classes)

        self.relu = nn.ReLU()

    def _get_cnn_output_size(self, seq_len, input_dim):
        # Calculate the output size of CNN layers
        x = torch.zeros(1, input_dim, seq_len)
        x = self.conv1(x)
        x = self.pool1(x)
        x = self.conv2(x)
        x = self.pool2(x)
        return x.numel()

    def forward(self, x):
        batch_size, seq_len, features = x.size()

        # CNN branch - needs [batch, channels, seq_len]
        x_cnn = x.permute(0, 2, 1)
        x_cnn = self.relu(self.conv1(x_cnn))
        x_cnn = self.pool1(x_cnn)
        x_cnn = self.relu(self.conv2(x_cnn))
        x_cnn = self.pool2(x_cnn)
        x_cnn = x_cnn.view(batch_size, -1)  # Flatten

        # LSTM branch
        lstm_out, _ = self.lstm(x)

        # Self-attention
        attn_out, self.attn_weights = self.attention(lstm_out)

        # Skip connection
        attn_out = lstm_out + attn_out
        attn_out = self.layer_norm(attn_out)

        # Flatten attention output
        attn_out = attn_out.reshape(batch_size, -1)

        # Concatenate CNN and LSTM branches
        combined = torch.cat((attn_out, x_cnn), dim=1)

        # Fully connected layers
        x = self.relu(self.fc1(combined))
        x = self.dropout1(x)
        x = self.relu(self.fc2(x))
        x = self.dropout2(x)
        x = self.fc3(x)

        return x

class HealthcareIoTAnalysis:
    def __init__(self, sequence_length=30, batch_size=32, epochs=500, learning_rate=0.001):
        """
        Initialize the Healthcare IoT Analysis system

        Parameters:
        -----------
        sequence_length : int
            Length of time series sequences (e.g., 30-minute intervals)
        batch_size : int
            Batch size for training
        epochs : int
            Maximum number of training epochs
        learning_rate : float
            Learning rate for optimizer
        """
        self.sequence_length = sequence_length
        self.batch_size = batch_size
        self.epochs = epochs
        self.learning_rate = learning_rate
        self.model = None
        self.scaler = MinMaxScaler()
        self.history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}
        self.attn_weights = None
        self.device = device

    def preprocess_data(self, df, target_col, feature_names=None):
        """
        Preprocess the data for time series analysis

        Parameters:
        -----------
        df : pandas DataFrame
            Input data
        target_col : str
            Name of the target column
        feature_names : list, optional
            Names of features to use

        Returns:
        --------
        X_train : numpy array
            Training data
        X_test : numpy array
            Test data
        y_train : numpy array
            Training labels
        y_test : numpy array
            Test labels
        feature_names : list
            Names of features used
        """
        print("Preprocessing data...")

        # If feature names not provided, use all columns except target
        if feature_names is None:
            feature_names = [col for col in df.columns if col != target_col]

        # Extract features and target
        X = df[feature_names].values
        y = df[target_col].values

        # Normalize features
        self.scaler = StandardScaler()
        X_scaled = self.scaler.fit_transform(X)

        # Create sequences
        X_sequences = []
        y_sequences = []

        for i in range(len(X_scaled) - self.sequence_length + 1):
            X_sequences.append(X_scaled[i:i+self.sequence_length])
            y_sequences.append(y[i+self.sequence_length-1])

        X_sequences = np.array(X_sequences)
        y_sequences = np.array(y_sequences)

        # Split into train and test sets
        X_train, X_test, y_train, y_test = train_test_split(
            X_sequences, y_sequences, test_size=0.2, random_state=42
        )

        return X_train, X_test, y_train, y_test, feature_names

    def _create_sequences(self, X, y):
        """Create sequences for time series analysis"""
        X_sequences, y_sequences = [], []

        for i in range(len(X) - self.sequence_length):
            X_sequences.append(X[i:i + self.sequence_length])
            # Use the label at the end of the sequence
            y_sequences.append(y[i + self.sequence_length - 1])

        return np.array(X_sequences), np.array(y_sequences)

    def _augment_rare_classes(self, X, y):
        """Augment rare classes to balance the dataset"""
        # Convert y to integers for bincount
        y_int = y.astype(int)
        class_counts = np.bincount(y_int)
        max_count = np.max(class_counts)

        # Find indices for each class
        class_indices = [np.where(y_int == i)[0] for i in range(len(class_counts))]

        X_augmented, y_augmented = X.copy(), y.copy()

        # Augment underrepresented classes
        for class_idx, indices in enumerate(class_indices):
            if len(indices) < max_count:
                # Number of samples to generate
                n_samples = max_count - len(indices)

                # Generate synthetic samples
                for _ in range(n_samples):
                    # Randomly select a sample from this class
                    idx = np.random.choice(indices)
                    sample = X[idx].copy()

                    # Add random noise to create a new sample
                    noise = np.random.normal(0, 0.05, sample.shape)
                    new_sample = sample + noise

                    # Add the new sample
                    X_augmented = np.vstack([X_augmented, [new_sample]])
                    y_augmented = np.append(y_augmented, class_idx)

        return X_augmented, y_augmented

    def build_model(self, input_shape, num_classes):
        """
        Build a bidirectional LSTM model with attention mechanisms

        Parameters:
        -----------
        input_shape : tuple
            Shape of input data (sequence_length, num_features)
        num_classes : int
            Number of classes for classification

        Returns:
        --------
        model : PyTorch model
            Compiled model
        """
        print("Building model...")

        # Extract dimensions
        _, input_dim = input_shape
        hidden_dim = 64
        num_layers = 2

        # Create model
        model = BiLSTMAttentionModel(
            input_dim=input_dim,
            hidden_dim=hidden_dim,
            num_layers=num_layers,
            num_classes=num_classes,
            dropout=0.5
        ).to(self.device)

        # Print model summary
        print(model)
        print(f"Number of parameters: {sum(p.numel() for p in model.parameters())}")

        self.model = model
        return model

    def train(self, X_train, y_train, X_val, y_val):
        """
        Train the model

        Parameters:
        -----------
        X_train, y_train : numpy arrays
            Training data
        X_val, y_val : numpy arrays
            Validation data

        Returns:
        --------
        history : dict
            Training history
        """
        print("Training model...")

        # Create datasets and dataloaders
        train_dataset = TimeSeriesDataset(X_train, y_train)
        val_dataset = TimeSeriesDataset(X_val, y_val)

        train_loader = DataLoader(
            train_dataset,
            batch_size=self.batch_size,
            shuffle=True
        )

        val_loader = DataLoader(
            val_dataset,
            batch_size=self.batch_size,
            shuffle=False
        )

        # Define loss function and optimizer
        criterion = nn.CrossEntropyLoss()  # Use CrossEntropyLoss for both binary and multiclass
        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)

        # For early stopping
        best_val_loss = float('inf')
        patience = 20
        patience_counter = 0

        # Training loop
        for epoch in range(self.epochs):
            # Training phase
            self.model.train()
            train_loss = 0.0
            train_correct = 0
            train_total = 0

            for inputs, targets in train_loader:
                inputs, targets = inputs.to(self.device), targets.to(self.device)

                # Forward pass
                optimizer.zero_grad()
                outputs = self.model(inputs)

                # Calculate loss and accuracy
                loss = criterion(outputs, targets)
                _, predicted = torch.max(outputs, 1)

                # Backward pass and optimize
                loss.backward()
                optimizer.step()

                # Track statistics
                train_loss += loss.item() * inputs.size(0)
                train_correct += (predicted == targets).sum().item()
                train_total += targets.size(0)

            train_loss = train_loss / len(train_loader.dataset)
            train_acc = train_correct / train_total

            # Validation phase
            self.model.eval()
            val_loss = 0.0
            val_correct = 0
            val_total = 0

            with torch.no_grad():
                for inputs, targets in val_loader:
                    inputs, targets = inputs.to(self.device), targets.to(self.device)

                    # Forward pass
                    outputs = self.model(inputs)

                    # Calculate loss and accuracy
                    loss = criterion(outputs, targets)
                    _, predicted = torch.max(outputs, 1)

                    # Track statistics
                    val_loss += loss.item() * inputs.size(0)
                    val_correct += (predicted == targets).sum().item()
                    val_total += targets.size(0)

            val_loss = val_loss / len(val_loader.dataset)
            val_acc = val_correct / val_total

            # Save history
            self.history['train_loss'].append(train_loss)
            self.history['train_acc'].append(train_acc)
            self.history['val_loss'].append(val_loss)
            self.history['val_acc'].append(val_acc)

            # Print progress
            if (epoch + 1) % 10 == 0:
                print(f'Epoch {epoch+1}/{self.epochs}, '
                      f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, '
                      f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')

            # Early stopping
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                # Save best model
                torch.save(self.model.state_dict(), 'best_model.pt')
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    print(f'Early stopping at epoch {epoch+1}')
                    break

        # Load best model
        self.model.load_state_dict(torch.load('best_model.pt'))
        return self.history

    def evaluate(self, X_test, y_test, show_plots=True):
        """
        Evaluate the model

        Parameters:
        -----------
        X_test, y_test : numpy arrays
            Testing data
        show_plots : bool
            Whether to show visualizations

        Returns:
        --------
        metrics : dict
            Evaluation metrics
        """
        print("Evaluating model...")

        # Create dataset and dataloader
        test_dataset = TimeSeriesDataset(X_test, y_test)
        test_loader = DataLoader(
            test_dataset,
            batch_size=self.batch_size,
            shuffle=False
        )

        # Set model to evaluation mode
        self.model.eval()

        # Make predictions
        all_preds = []
        all_probs = []

        with torch.no_grad():
            for inputs, _ in test_loader:
                inputs = inputs.to(self.device)
                outputs = self.model(inputs)

                # Handle binary vs multiclass
                if outputs.shape[1] == 1:  # Binary
                    probs = torch.sigmoid(outputs).cpu().numpy()
                    preds = (probs > 0.5).astype(int)
                else:  # Multiclass
                    probs = torch.softmax(outputs, dim=1).cpu().numpy()
                    preds = np.argmax(probs, axis=1)

                all_preds.extend(preds)
                all_probs.extend(probs)

        # Convert to numpy arrays
        y_pred = np.array(all_preds).flatten()
        y_pred_proba = np.array(all_probs)

        # Calculate metrics
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, average='weighted')
        recall = recall_score(y_test, y_pred, average='weighted')
        f1 = f1_score(y_test, y_pred, average='weighted')

        # Create confusion matrix
        cm = confusion_matrix(y_test, y_pred)

        # Print metrics
        print(f"Accuracy: {accuracy:.4f}")
        print(f"Precision: {precision:.4f}")
        print(f"Recall: {recall:.4f}")
        print(f"F1 Score: {f1:.4f}")

        # Plot confusion matrix
        if show_plots:
            plt.figure(figsize=(10, 8))
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
            plt.title('Confusion Matrix')
            plt.ylabel('True Label')
            plt.xlabel('Predicted Label')
            plt.show()

        # Plot training history
        self._plot_training_history()

        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'confusion_matrix': cm,
            'y_pred': y_pred,
            'y_pred_proba': y_pred_proba
        }

    def _plot_training_history(self):
        """Plot training history"""
        plt.figure(figsize=(12, 5))

        # Plot accuracy
        plt.subplot(1, 2, 1)
        plt.plot(self.history['train_acc'], label='Training Accuracy')
        plt.plot(self.history['val_acc'], label='Validation Accuracy')
        plt.title('Model Accuracy')
        plt.xlabel('Epoch')
        plt.ylabel('Accuracy')
        plt.legend()

        # Plot loss
        plt.subplot(1, 2, 2)
        plt.plot(self.history['train_loss'], label='Training Loss')
        plt.plot(self.history['val_loss'], label='Validation Loss')
        plt.title('Model Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()

        plt.tight_layout()
        plt.show()

    def visualize_attention(self, X_sample, feature_names):
        """
        Visualize attention weights to interpret model decisions

        Parameters:
        -----------
        X_sample : numpy array
            Sample input data
        feature_names : list
            Names of features
        """
        # Convert to tensor and move to device
        X_tensor = torch.tensor(X_sample, dtype=torch.float32).unsqueeze(0).to(self.device)

        # Set model to evaluation mode
        self.model.eval()

        # Forward pass to get attention weights
        with torch.no_grad():
            _ = self.model(X_tensor)
            attention_weights = self.model.attn_weights.cpu().numpy()

        # Average attention weights across heads
        avg_attention = np.mean(attention_weights, axis=0)

        # Plot attention heatmap
        plt.figure(figsize=(12, 8))
        sns.heatmap(
            avg_attention,
            xticklabels=feature_names,
            yticklabels=range(self.sequence_length),
            cmap='viridis'
        )
        plt.title('Attention Weights')
        plt.xlabel('Features')
        plt.ylabel('Time Steps')
        plt.show()

        return avg_attention

    def create_dashboard(self, X_test, y_test, y_pred, feature_names):
        """
        Create an interactive dashboard for real-time monitoring
        """
        # Create a Dash app
        app = dash.Dash(__name__, suppress_callback_exceptions=True)

        # Define layout
        app.layout = html.Div([
            html.H1("Healthcare IoT Monitoring Dashboard"),

            html.Div([
                html.H3("Patient Selection"),
                dcc.Dropdown(
                    id='patient-dropdown',
                    options=[{'label': f'Patient {i}', 'value': i} for i in range(min(10, len(X_test)))],
                    value=0
                )
            ]),

            html.Div([
                html.Div([
                    html.H3("Vital Signs"),
                    dcc.Graph(id='vitals-graph')
                ], style={'width': '70%', 'display': 'inline-block'}),

                html.Div([
                    html.H3("Prediction"),
                    html.Div(id='prediction-result', style={
                        'fontSize': '24px',
                        'fontWeight': 'bold',
                        'textAlign': 'center',
                        'marginTop': '50px'
                    })
                ], style={'width': '30%', 'display': 'inline-block', 'verticalAlign': 'top'})
            ]),

            html.Div([
                html.H3("Feature Importance"),
                dcc.Graph(id='feature-importance')
            ]),

            html.Div([
                html.H3("Anomaly Detection"),
                dcc.Graph(id='anomaly-graph')
            ])
        ])

        # Define callbacks
        @app.callback(
            [Output('vitals-graph', 'figure'),
             Output('prediction-result', 'children'),
             Output('prediction-result', 'style'),
             Output('feature-importance', 'figure'),
             Output('anomaly-graph', 'figure')],
            [DashInput('patient-dropdown', 'value')]
        )
        def update_graphs(patient_idx):
            try:
                # Get patient data
                patient_data = X_test[patient_idx]
                true_label = y_test[patient_idx]
                pred_label = y_pred[patient_idx]

                # Create vitals graph
                vitals_fig = go.Figure()
                for i, feature in enumerate(feature_names):
                    vitals_fig.add_trace(go.Scatter(
                        x=list(range(self.sequence_length)),
                        y=patient_data[:, i],
                        mode='lines',
                        name=feature
                    ))
                vitals_fig.update_layout(
                    title='Patient Vital Signs Over Time',
                    xaxis_title='Time Steps',
                    yaxis_title='Normalized Value'
                )

                # Create prediction result
                if true_label == pred_label:
                    result_style = {
                        'fontSize': '24px',
                        'fontWeight': 'bold',
                        'textAlign': 'center',
                        'marginTop': '50px',
                        'color': 'green'
                    }
                    result_text = f"Correct Prediction: {pred_label}"
                else:
                    result_style = {
                        'fontSize': '24px',
                        'fontWeight': 'bold',
                        'textAlign': 'center',
                        'marginTop': '50px',
                        'color': 'red'
                    }
                    result_text = f"Incorrect Prediction: {pred_label} (True: {true_label})"

                # Simplified feature importance calculation
                # Instead of using attention weights directly, use a simpler approach
                feature_importance = np.std(patient_data, axis=0)  # Use standard deviation as importance

                importance_fig = px.bar(
                    x=feature_names,
                    y=feature_importance,
                    title='Feature Importance (Based on Variability)'
                )

                # Create anomaly graph
                # Calculate z-scores to detect anomalies
                z_scores = np.abs((patient_data - np.mean(patient_data, axis=0)) / np.std(patient_data, axis=0))
                anomaly_threshold = 2.0  # Z-score threshold for anomalies

                anomaly_fig = go.Figure()
                for i, feature in enumerate(feature_names):
                    # Add normal points
                    normal_indices = np.where(z_scores[:, i] <= anomaly_threshold)[0]
                    if len(normal_indices) > 0:
                        anomaly_fig.add_trace(go.Scatter(
                            x=normal_indices,
                            y=patient_data[normal_indices, i],
                            mode='lines+markers',
                            name=f'{feature} (Normal)',
                            line=dict(color='blue')
                        ))

                    # Add anomaly points
                    anomaly_indices = np.where(z_scores[:, i] > anomaly_threshold)[0]
                    if len(anomaly_indices) > 0:
                        anomaly_fig.add_trace(go.Scatter(
                            x=anomaly_indices,
                            y=patient_data[anomaly_indices, i],
                            mode='markers',
                            name=f'{feature} (Anomaly)',
                            marker=dict(color='red', size=10, symbol='x')
                        ))

                anomaly_fig.update_layout(
                    title='Anomaly Detection in Patient Data',
                    xaxis_title='Time Steps',
                    yaxis_title='Normalized Value'
                )

                return vitals_fig, result_text, result_style, importance_fig, anomaly_fig

            except Exception as e:
                print(f"Error in dashboard callback: {e}")
                # Return empty figures in case of error
                empty_fig = go.Figure()
                empty_fig.update_layout(title="Error loading data")
                default_style = {'color': 'black', 'fontSize': '24px', 'textAlign': 'center'}
                return empty_fig, f"Error: {str(e)}", default_style, empty_fig, empty_fig

        return app

    def visualize_patient(self, X_test, y_test, y_pred, patient_idx, feature_names):
        """Simple visualization alternative to dashboard"""
        patient_data = X_test[patient_idx]
        true_label = y_test[patient_idx]
        pred_label = y_pred[patient_idx]

        # Create figure with subplots
        fig, axs = plt.subplots(3, 1, figsize=(15, 15))

        # Plot vital signs
        for i, feature in enumerate(feature_names):
            axs[0].plot(patient_data[:, i], label=feature)
        axs[0].set_title(f'Patient {patient_idx} Vital Signs')
        axs[0].set_xlabel('Time Steps')
        axs[0].set_ylabel('Normalized Value')
        axs[0].legend()

        # Plot feature importance
        feature_importance = np.std(patient_data, axis=0)
        axs[1].bar(feature_names, feature_importance)
        axs[1].set_title('Feature Importance')
        axs[1].set_xlabel('Features')
        axs[1].set_ylabel('Importance')
        plt.setp(axs[1].get_xticklabels(), rotation=45, ha='right')

        # Add prediction result as text
        result_text = f"Prediction: {pred_label} (True: {true_label})"
        correct = pred_label == true_label
        color = 'green' if correct else 'red'
        axs[1].text(0.5, -0.2, result_text, transform=axs[1].transAxes,
                    ha='center', fontsize=14, color=color)

        # Plot anomalies
        z_scores = np.abs((patient_data - np.mean(patient_data, axis=0)) / np.std(patient_data, axis=0))
        anomaly_threshold = 2.0

        for i, feature in enumerate(feature_names):
            # Plot normal points
            normal_indices = np.where(z_scores[:, i] <= anomaly_threshold)[0]
            axs[2].plot(normal_indices, patient_data[normal_indices, i], 'o-', label=f'{feature} (Normal)')

            # Plot anomaly points
            anomaly_indices = np.where(z_scores[:, i] > anomaly_threshold)[0]
            if len(anomaly_indices) > 0:
                axs[2].plot(anomaly_indices, patient_data[anomaly_indices, i], 'rx', markersize=10, label=f'{feature} (Anomaly)')

        axs[2].set_title('Anomaly Detection')
        axs[2].set_xlabel('Time Steps')
        axs[2].set_ylabel('Normalized Value')

        plt.tight_layout()
        plt.show()

    def predict_future_trends(self, patient_data, feature_names, steps_ahead=10):
        """
        Predict future trends for a patient's vital signs

        Parameters:
        -----------
        patient_data : numpy array
            Patient's historical data
        feature_names : list
            Names of features
        steps_ahead : int
            Number of steps to predict ahead

        Returns:
        --------
        predictions : numpy array
            Predicted values for each feature
        """
        print("Predicting future trends...")

        # Create a copy of the patient data
        data_sequence = patient_data.copy()
        predictions = []

        # Convert to tensor
        data_tensor = torch.tensor(data_sequence, dtype=torch.float32).unsqueeze(0).to(self.device)

        # Set model to evaluation mode
        self.model.eval()

        # Make predictions for each step ahead
        for _ in range(steps_ahead):
            with torch.no_grad():
                # Get model output
                output = self.model(data_tensor)

                # Create a synthetic next step based on the last step and prediction
                last_step = data_sequence[-1].copy()

                # Adjust values based on prediction (simplified approach)
                # In a real system, you would use a more sophisticated method
                if torch.argmax(output).item() == 1:  # If predicted abnormal
                    # Simulate worsening condition
                    last_step[0] += 0.05  # Heart rate increases
                    last_step[2] += 0.03  # Temperature increases
                    last_step[3] -= 0.04  # Oxygen saturation decreases
                else:
                    # Simulate stable condition
                    last_step += np.random.normal(0, 0.02, last_step.shape)

                # Add the prediction
                predictions.append(last_step)

                # Update the sequence by removing the first step and adding the new one
                data_sequence = np.vstack([data_sequence[1:], last_step])
                data_tensor = torch.tensor(data_sequence, dtype=torch.float32).unsqueeze(0).to(self.device)

        return np.array(predictions)

    def calculate_risk_score(self, patient_data, feature_names):
        """
        Calculate a risk score for the patient based on vital signs

        Parameters:
        -----------
        patient_data : numpy array
            Patient's data
        feature_names : list
            Names of features

        Returns:
        --------
        risk_score : float
            Risk score between 0 and 100
        risk_factors : list
            List of contributing risk factors
        """
        # Convert to tensor
        data_tensor = torch.tensor(patient_data, dtype=torch.float32).unsqueeze(0).to(self.device)

        # Get model prediction and attention weights
        self.model.eval()
        with torch.no_grad():
            output = self.model(data_tensor)
            prob = torch.softmax(output, dim=1)[0, 1].item()  # Probability of abnormal class

        # Calculate z-scores to identify abnormal values
        z_scores = np.abs((patient_data - np.mean(patient_data, axis=0)) / np.std(patient_data, axis=0))

        # Identify risk factors (features with high z-scores)
        risk_threshold = 2.0
        risk_factors = []
        for i, feature in enumerate(feature_names):
            max_z = np.max(z_scores[:, i])
            if max_z > risk_threshold:
                risk_factors.append((feature, max_z))

        # Sort risk factors by severity
        risk_factors.sort(key=lambda x: x[1], reverse=True)

        # Calculate overall risk score (0-100)
        # Combine model prediction and z-scores
        model_weight = 0.7
        zscore_weight = 0.3

        model_score = prob * 100

        # Handle case when no risk factors are found
        if risk_factors:
            zscore_score = min(100, np.mean([factor[1] for factor in risk_factors]) * 20)
        else:
            zscore_score = 0

        risk_score = (model_weight * model_score) + (zscore_weight * zscore_score)

        return risk_score, risk_factors

    def visualize_comprehensive_analysis(self, X_test, y_test, y_pred, patient_idx, feature_names):
        """
        Create a comprehensive visualization of patient analysis

        Parameters:
        -----------
        X_test : numpy array
            Test data
        y_test : numpy array
            True labels
        y_pred : numpy array
            Predicted labels
        patient_idx : int
            Index of the patient to visualize
        feature_names : list
            Names of features
        """
        patient_data = X_test[patient_idx]
        true_label = y_test[patient_idx]
        pred_label = y_pred[patient_idx]

        # Create figure with subplots
        fig = plt.figure(figsize=(20, 20))
        gs = fig.add_gridspec(4, 2)

        # 1. Vital Signs Plot
        ax1 = fig.add_subplot(gs[0, :])
        for i, feature in enumerate(feature_names):
            ax1.plot(patient_data[:, i], label=feature)
        ax1.set_title(f'Patient {patient_idx} Vital Signs', fontsize=16)
        ax1.set_xlabel('Time Steps')
        ax1.set_ylabel('Normalized Value')
        ax1.legend(loc='upper right')

        # Add prediction result as text
        result_text = f"Prediction: {pred_label} (True: {true_label})"
        correct = pred_label == true_label
        color = 'green' if correct else 'red'
        ax1.text(0.02, 0.95, result_text, transform=ax1.transAxes,
                 fontsize=14, color=color, bbox=dict(facecolor='white', alpha=0.8))

        # 2. Feature Importance
        ax2 = fig.add_subplot(gs[1, 0])
        feature_importance = np.std(patient_data, axis=0)
        sorted_idx = np.argsort(feature_importance)
        ax2.barh([feature_names[i] for i in sorted_idx], feature_importance[sorted_idx])
        ax2.set_title('Feature Importance', fontsize=16)
        ax2.set_xlabel('Importance Score')

        # 3. Anomaly Detection
        ax3 = fig.add_subplot(gs[1, 1])
        z_scores = np.abs((patient_data - np.mean(patient_data, axis=0)) / np.std(patient_data, axis=0))
        anomaly_threshold = 2.0

        # Create heatmap of z-scores
        im = ax3.imshow(z_scores.T, aspect='auto', cmap='YlOrRd')
        ax3.set_title('Anomaly Detection (Z-scores)', fontsize=16)
        ax3.set_xlabel('Time Steps')
        ax3.set_yticks(range(len(feature_names)))
        ax3.set_yticklabels(feature_names)
        plt.colorbar(im, ax=ax3, label='Z-score')

        # 4. Future Trend Prediction
        ax4 = fig.add_subplot(gs[2, :])
        future_predictions = self.predict_future_trends(patient_data, feature_names)

        # Plot historical data (solid lines)
        for i, feature in enumerate(feature_names):
            ax4.plot(range(len(patient_data)), patient_data[:, i],
                     label=feature, linestyle='-')

        # Plot predictions (dashed lines)
        for i, feature in enumerate(feature_names):
            ax4.plot(range(len(patient_data), len(patient_data) + len(future_predictions)),
                     future_predictions[:, i], linestyle='--', color=f'C{i}')

        # Add vertical line to separate historical and predicted data
        ax4.axvline(x=len(patient_data)-1, color='black', linestyle=':')
        ax4.text(len(patient_data), -0.5, 'Prediction Start', ha='center')

        ax4.set_title('Future Trend Prediction', fontsize=16)
        ax4.set_xlabel('Time Steps')
        ax4.set_ylabel('Normalized Value')

        # 5. Risk Assessment - Fixed version using a simple gauge
        ax5 = fig.add_subplot(gs[3, 0])
        risk_score, risk_factors = self.calculate_risk_score(patient_data, feature_names)

        # Create a simple gauge chart using a semicircle
        risk_score_normalized = risk_score / 100.0  # Normalize to 0-1

        # Draw background semicircle
        theta = np.linspace(0, np.pi, 100)
        r = 0.8

        # Low risk (green)
        low_idx = np.where(theta <= np.pi * 0.3)[0]
        ax5.plot(r * np.cos(theta[low_idx]), r * np.sin(theta[low_idx]), 'g-', linewidth=20, alpha=0.3)

        # Medium risk (yellow)
        med_idx = np.where((theta > np.pi * 0.3) & (theta <= np.pi * 0.7))[0]
        ax5.plot(r * np.cos(theta[med_idx]), r * np.sin(theta[med_idx]), 'y-', linewidth=20, alpha=0.3)

        # High risk (red)
        high_idx = np.where(theta > np.pi * 0.7)[0]
        ax5.plot(r * np.cos(theta[high_idx]), r * np.sin(theta[high_idx]), 'r-', linewidth=20, alpha=0.3)

        # Draw needle
        needle_theta = np.pi * risk_score_normalized
        ax5.plot([0, r * np.cos(needle_theta)], [0, r * np.sin(needle_theta)], 'k-', linewidth=3)

        # Add risk score text
        ax5.text(0, -0.2, f"Risk Score: {risk_score:.1f}%", ha='center', va='center',
                 fontsize=16, fontweight='bold')

        # Set up the plot
        ax5.set_title('Patient Risk Assessment', fontsize=16)
        ax5.set_xlim(-1, 1)
        ax5.set_ylim(-0.5, 1)
        ax5.axis('off')

        # 6. Risk Factors
        ax6 = fig.add_subplot(gs[3, 1])
        if risk_factors:
            factors, scores = zip(*risk_factors)
            ax6.barh(factors, scores, color='tomato')
            ax6.set_title('Contributing Risk Factors', fontsize=16)
            ax6.set_xlabel('Severity (Z-score)')
        else:
            ax6.text(0.5, 0.5, "No significant risk factors detected",
                     ha='center', va='center', fontsize=14)
            ax6.set_title('Contributing Risk Factors', fontsize=16)
            ax6.axis('off')

        plt.tight_layout()
        plt.show()

    def generate_patient_report(self, X_test, y_test, y_pred, patient_idx, feature_names):
        """
        Generate a textual report for a patient

        Parameters:
        -----------
        X_test : numpy array
            Test data
        y_test : numpy array
            True labels
        y_pred : numpy array
            Predicted labels
        patient_idx : int
            Index of the patient to analyze
        feature_names : list
            Names of features

        Returns:
        --------
        report : str
            Textual report
        """
        patient_data = X_test[patient_idx]
        true_label = y_test[patient_idx]
        pred_label = y_pred[patient_idx]

        # Calculate risk score and factors
        risk_score, risk_factors = self.calculate_risk_score(patient_data, feature_names)

        # Calculate z-scores to identify abnormal values
        z_scores = np.abs((patient_data - np.mean(patient_data, axis=0)) / np.std(patient_data, axis=0))
        max_z_scores = np.max(z_scores, axis=0)

        # Generate report
        report = f"PATIENT {patient_idx} HEALTH REPORT\n"
        report += "=" * 50 + "\n\n"

        # Overall status
        if pred_label == 1:
            report += "ALERT: Abnormal patterns detected in patient data.\n"
        else:
            report += "STATUS: Normal patterns observed in patient data.\n"

        report += f"Risk Score: {risk_score:.1f}%\n\n"

        # Risk level categorization
        if risk_score < 30:
            report += "Risk Level: LOW - Routine monitoring recommended.\n"
        elif risk_score < 70:
            report += "Risk Level: MODERATE - Increased monitoring recommended.\n"
        else:
            report += "Risk Level: HIGH - Immediate attention recommended.\n"

        report += "\nVITAL SIGNS SUMMARY:\n"
        report += "-" * 50 + "\n"

        # Summarize each vital sign
        for i, feature in enumerate(feature_names):
            avg_value = np.mean(patient_data[:, i])
            max_z = max_z_scores[i]

            report += f"{feature}: "

            if max_z > 2.5:
                report += f"CRITICAL (z-score: {max_z:.2f})\n"
            elif max_z > 2.0:
                report += f"ABNORMAL (z-score: {max_z:.2f})\n"
            elif max_z > 1.5:
                report += f"BORDERLINE (z-score: {max_z:.2f})\n"
            else:
                report += f"NORMAL (z-score: {max_z:.2f})\n"

        report += "\nRISK FACTORS:\n"
        report += "-" * 50 + "\n"

        if risk_factors:
            for factor, score in risk_factors:
                report += f"- {factor}: Severity score {score:.2f}\n"
        else:
            report += "No significant risk factors detected.\n"

        # Add recommendations
        report += "\nRECOMMENDATIONS:\n"
        report += "-" * 50 + "\n"

        if risk_score >= 70:
            report += "1. Immediate medical attention recommended.\n"
            report += "2. Continuous monitoring of vital signs.\n"
            report += "3. Prepare for potential intervention.\n"
        elif risk_score >= 30:
            report += "1. Increase monitoring frequency.\n"
            report += "2. Review medication and treatment plan.\n"
            report += "3. Schedule follow-up assessment within 24 hours.\n"
        else:
            report += "1. Continue routine monitoring.\n"
            report += "2. No immediate action required.\n"
            report += "3. Next assessment as regularly scheduled.\n"

        # Add prediction accuracy note
        if pred_label == true_label:
            report += "\nNote: The AI prediction system correctly identified the patient's condition.\n"
        else:
            report += "\nNote: The AI prediction system's assessment differs from the known condition. Human verification recommended.\n"

        return report

    def analyze_population_trends(self, X_test, y_test, y_pred, feature_names):
        """
        Analyze trends across the entire patient population

        Parameters:
        -----------
        X_test : numpy array
            Test data
        y_test : numpy array
            True labels
        y_pred : numpy array
            Predicted labels
        feature_names : list
            Names of features
        """
        # Create figure with subplots
        fig = plt.figure(figsize=(20, 15))
        gs = fig.add_gridspec(3, 2)

        # 1. Distribution of risk scores
        ax1 = fig.add_subplot(gs[0, 0])
        risk_scores = []

        for i in range(len(X_test)):
            score, _ = self.calculate_risk_score(X_test[i], feature_names)
            risk_scores.append(score)

        ax1.hist(risk_scores, bins=20, color='skyblue', edgecolor='black')
        ax1.set_title('Distribution of Patient Risk Scores', fontsize=16)
        ax1.set_xlabel('Risk Score (%)')
        ax1.set_ylabel('Number of Patients')

        # Add vertical lines for risk categories
        ax1.axvline(x=30, color='green', linestyle='--', label='Low Risk Threshold')
        ax1.axvline(x=70, color='red', linestyle='--', label='High Risk Threshold')
        ax1.legend()

        # 2. Feature correlation heatmap
        ax2 = fig.add_subplot(gs[0, 1])

        # Flatten all patient data
        all_data = X_test.reshape(-1, X_test.shape[2])

        # Calculate correlation matrix
        corr_matrix = np.corrcoef(all_data.T)

        # Create heatmap
        im = ax2.imshow(corr_matrix, cmap='coolwarm', vmin=-1, vmax=1)
        ax2.set_title('Feature Correlation Matrix', fontsize=16)

        # Add feature names as ticks
        ax2.set_xticks(range(len(feature_names)))
        ax2.set_yticks(range(len(feature_names)))
        ax2.set_xticklabels(feature_names, rotation=45, ha='right')
        ax2.set_yticklabels(feature_names)

        # Add colorbar
        plt.colorbar(im, ax=ax2, label='Correlation Coefficient')

        # 3. Anomaly frequency by feature
        ax3 = fig.add_subplot(gs[1, 0])

        # Calculate z-scores for all patients
        anomaly_counts = np.zeros(len(feature_names))

        for i in range(len(X_test)):
            z_scores = np.abs((X_test[i] - np.mean(X_test[i], axis=0)) / np.std(X_test[i], axis=0))
            # Count anomalies (z-score > 2.0)
            anomalies = (z_scores > 2.0).sum(axis=0)
            anomaly_counts += anomalies

        # Sort features by anomaly frequency
        sorted_idx = np.argsort(anomaly_counts)
        ax3.barh([feature_names[i] for i in sorted_idx], anomaly_counts[sorted_idx], color='salmon')
        ax3.set_title('Frequency of Anomalies by Feature', fontsize=16)
        ax3.set_xlabel('Number of Anomalies Detected')

        # 4. Model performance metrics
        ax4 = fig.add_subplot(gs[1, 1])

        # Calculate metrics
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, average='weighted')
        recall = recall_score(y_test, y_pred, average='weighted')
        f1 = f1_score(y_test, y_pred, average='weighted')

        metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']
        values = [accuracy, precision, recall, f1]

        ax4.bar(metrics, values, color='lightgreen')
        ax4.set_title('Model Performance Metrics', fontsize=16)
        ax4.set_ylim(0, 1)

        # Add values on top of bars
        for i, v in enumerate(values):
            ax4.text(i, v + 0.02, f'{v:.3f}', ha='center')

        # 5. Risk distribution by predicted class
        ax5 = fig.add_subplot(gs[2, :])

        # Separate risk scores by predicted class
        normal_risks = [risk_scores[i] for i in range(len(risk_scores)) if y_pred[i] == 0]
        abnormal_risks = [risk_scores[i] for i in range(len(risk_scores)) if y_pred[i] == 1]

        # Create violin plot
        violin_parts = ax5.violinplot(
            [normal_risks, abnormal_risks],
            showmeans=True,
            showmedians=True
        )

        # Customize violin plot
        for pc in violin_parts['bodies']:
            pc.set_facecolor('lightblue')
            pc.set_edgecolor('black')
            pc.set_alpha(0.7)

        # Add scatter points for individual patients
        for i, data in enumerate([normal_risks, abnormal_risks]):
            # Add jittered points
            x = np.random.normal(i+1, 0.05, size=len(data))
            ax5.scatter(x, data, alpha=0.4, s=5, c='blue')

        ax5.set_title('Risk Score Distribution by Predicted Class', fontsize=16)
        ax5.set_xticks([1, 2])
        ax5.set_xticklabels(['Normal', 'Abnormal'])
        ax5.set_ylabel('Risk Score (%)')

        # Add horizontal lines for risk categories
        ax5.axhline(y=30, color='green', linestyle='--', label='Low Risk Threshold')
        ax5.axhline(y=70, color='red', linestyle='--', label='High Risk Threshold')
        ax5.legend()

        plt.tight_layout()
        plt.show()

    def assess_custom_patient(self, patient_parameters, feature_names):
        """
        Assess risk for a patient with custom input parameters

        Parameters:
        -----------
        patient_parameters : dict
            Dictionary with feature names as keys and lists of values as values
            Example: {'Heart_Rate': [75, 80, 85], 'Blood_Pressure': [120, 125, 130], ...}
        feature_names : list
            Names of features in the correct order

        Returns:
        --------
        risk_assessment : dict
            Dictionary containing risk assessment results
        """
        print("Assessing custom patient data...")

        # Convert input dictionary to numpy array
        sequence_length = len(list(patient_parameters.values())[0])  # Get length from first parameter
        patient_data = np.zeros((sequence_length, len(feature_names)))

        for i, feature in enumerate(feature_names):
            if feature in patient_parameters:
                patient_data[:, i] = patient_parameters[feature]
            else:
                print(f"Warning: Feature '{feature}' not provided. Using zeros.")

        # Normalize the data using the same scaler used for training
        patient_data_normalized = self.scaler.transform(patient_data)

        # Reshape for model input (add batch dimension)
        patient_sequence = patient_data_normalized.reshape(1, sequence_length, -1)

        # Convert to tensor
        data_tensor = torch.tensor(patient_sequence, dtype=torch.float32).to(self.device)

        # Get model prediction
        self.model.eval()
        with torch.no_grad():
            output = self.model(data_tensor)
            probabilities = torch.softmax(output, dim=1)[0].cpu().numpy()
            prediction = torch.argmax(output, dim=1).item()

        # Calculate risk score and factors
        risk_score, risk_factors = self.calculate_risk_score(patient_data_normalized, feature_names)

        # Calculate z-scores for anomaly detection
        z_scores = np.abs((patient_data_normalized - np.mean(patient_data_normalized, axis=0)) /
                          np.std(patient_data_normalized, axis=0))
        max_z_scores = np.max(z_scores, axis=0)

        # Prepare the assessment results
        assessment = {
            'prediction': 'Abnormal' if prediction == 1 else 'Normal',
            'abnormal_probability': float(probabilities[1]),
            'risk_score': float(risk_score),
            'risk_level': 'HIGH' if risk_score >= 70 else 'MODERATE' if risk_score >= 30 else 'LOW',
            'risk_factors': risk_factors,
            'vital_signs_status': {}
        }

        # Assess each vital sign
        for i, feature in enumerate(feature_names):
            max_z = max_z_scores[i]
            if max_z > 2.5:
                status = 'CRITICAL'
            elif max_z > 2.0:
                status = 'ABNORMAL'
            elif max_z > 1.5:
                status = 'BORDERLINE'
            else:
                status = 'NORMAL'

            assessment['vital_signs_status'][feature] = {
                'status': status,
                'z_score': float(max_z),
                'values': patient_parameters.get(feature, [0] * sequence_length)
            }

        # Generate recommendations
        if risk_score >= 70:
            recommendations = [
                "Immediate medical attention recommended.",
                "Continuous monitoring of vital signs.",
                "Prepare for potential intervention."
            ]
        elif risk_score >= 30:
            recommendations = [
                "Increase monitoring frequency.",
                "Review medication and treatment plan.",
                "Schedule follow-up assessment within 24 hours."
            ]
        else:
            recommendations = [
                "Continue routine monitoring.",
                "No immediate action required.",
                "Next assessment as regularly scheduled."
            ]

        assessment['recommendations'] = recommendations

        # Visualize the assessment
        self.visualize_custom_assessment(patient_data_normalized, assessment, feature_names)

        return assessment

    def visualize_custom_assessment(self, patient_data, assessment, feature_names):
        """
        Visualize the assessment for a custom patient

        Parameters:
        -----------
        patient_data : numpy array
            Normalized patient data
        assessment : dict
            Assessment results from assess_custom_patient
        feature_names : list
            Names of features
        """
        # Create figure with subplots
        fig = plt.figure(figsize=(20, 15))
        gs = fig.add_gridspec(3, 2)

        # 1. Vital Signs Plot
        ax1 = fig.add_subplot(gs[0, :])
        for i, feature in enumerate(feature_names):
            ax1.plot(patient_data[:, i], label=feature)
        ax1.set_title('Patient Vital Signs', fontsize=16)
        ax1.set_xlabel('Time Steps')
        ax1.set_ylabel('Normalized Value')
        ax1.legend(loc='upper right')

        # Add prediction result as text
        result_text = f"Prediction: {assessment['prediction']} (Probability: {assessment['abnormal_probability']:.2f})"
        color = 'red' if assessment['prediction'] == 'Abnormal' else 'green'
        ax1.text(0.02, 0.95, result_text, transform=ax1.transAxes,
                 fontsize=14, color=color, bbox=dict(facecolor='white', alpha=0.8))

        # 2. Risk Assessment - Simple gauge
        ax2 = fig.add_subplot(gs[1, 0])
        risk_score = assessment['risk_score']

        # Create a simple gauge chart using a semicircle
        risk_score_normalized = risk_score / 100.0  # Normalize to 0-1

        # Draw background semicircle
        theta = np.linspace(0, np.pi, 100)
        r = 0.8

        # Low risk (green)
        low_idx = np.where(theta <= np.pi * 0.3)[0]
        ax2.plot(r * np.cos(theta[low_idx]), r * np.sin(theta[low_idx]), 'g-', linewidth=20, alpha=0.3)

        # Medium risk (yellow)
        med_idx = np.where((theta > np.pi * 0.3) & (theta <= np.pi * 0.7))[0]
        ax2.plot(r * np.cos(theta[med_idx]), r * np.sin(theta[med_idx]), 'y-', linewidth=20, alpha=0.3)

        # High risk (red)
        high_idx = np.where(theta > np.pi * 0.7)[0]
        ax2.plot(r * np.cos(theta[high_idx]), r * np.sin(theta[high_idx]), 'r-', linewidth=20, alpha=0.3)

        # Draw needle
        needle_theta = np.pi * risk_score_normalized
        ax2.plot([0, r * np.cos(needle_theta)], [0, r * np.sin(needle_theta)], 'k-', linewidth=3)

        # Add risk score text
        ax2.text(0, -0.2, f"Risk Score: {risk_score:.1f}% ({assessment['risk_level']})",
                 ha='center', va='center', fontsize=16, fontweight='bold')

        # Set up the plot
        ax2.set_title('Patient Risk Assessment', fontsize=16)
        ax2.set_xlim(-1, 1)
        ax2.set_ylim(-0.5, 1)
        ax2.axis('off')

        # 3. Risk Factors
        ax3 = fig.add_subplot(gs[1, 1])
        if assessment['risk_factors']:
            factors, scores = zip(*assessment['risk_factors'])
            ax3.barh(factors, scores, color='tomato')
            ax3.set_title('Contributing Risk Factors', fontsize=16)
            ax3.set_xlabel('Severity (Z-score)')
        else:
            ax3.text(0.5, 0.5, "No significant risk factors detected",
                     ha='center', va='center', fontsize=14)
            ax3.set_title('Contributing Risk Factors', fontsize=16)
            ax3.axis('off')

        # 4. Vital Signs Status
        ax4 = fig.add_subplot(gs[2, :])

        # Create a table of vital signs status
        vital_statuses = []
        vital_names = []
        vital_colors = []

        for feature in feature_names:
            vital_info = assessment['vital_signs_status'][feature]
            vital_names.append(feature)
            vital_statuses.append(f"{vital_info['status']} (z-score: {vital_info['z_score']:.2f})")

            if vital_info['status'] == 'CRITICAL':
                vital_colors.append('darkred')
            elif vital_info['status'] == 'ABNORMAL':
                vital_colors.append('red')
            elif vital_info['status'] == 'BORDERLINE':
                vital_colors.append('orange')
            else:
                vital_colors.append('green')

        # Create table
        table = ax4.table(
            cellText=[[status] for status in vital_statuses],
            rowLabels=vital_names,
            colLabels=['Status'],
            loc='center',
            cellLoc='center'
        )

        # Style the table
        table.auto_set_font_size(False)
        table.set_fontsize(12)
        table.scale(1, 2)

        # Color the cells based on status
        for i, color in enumerate(vital_colors):
            table[(i+1, 0)].set_text_props(color=color, fontweight='bold')

        ax4.set_title('Vital Signs Status', fontsize=16)
        ax4.axis('off')

        # Add recommendations
        recommendations = assessment['recommendations']
        rec_text = "RECOMMENDATIONS:\n" + "\n".join([f" {rec}" for rec in recommendations])
        plt.figtext(0.5, 0.02, rec_text, ha='center', fontsize=14,
                    bbox=dict(facecolor='lightyellow', alpha=0.5, boxstyle='round,pad=1'))

        plt.tight_layout(rect=[0, 0.05, 1, 0.95])
        plt.show()

# Example usage
def main():
    """
    Main function to run the healthcare IoT analysis
    """
    print("Loading healthcare IoT dataset...")
    # Load the CSV file
    df = pd.read_csv('healthcare_iot_target_dataset.csv')

    # Extract feature names
    feature_names = [col for col in df.columns if col not in ['Patient_ID', 'Timestamp', 'Sensor_ID', 'Sensor_Type', 'Target_Blood_Pressure', 'Target_Heart_Rate', 'Target_Health_Status']]

    # Use 'Target_Health_Status' as the target variable
    # Convert to binary: 1 for 'Unhealthy', 0 for 'Healthy'
    df['Target'] = (df['Target_Health_Status'] == 'Unhealthy').astype(int)

    print("Initializing Healthcare IoT Analysis System...")
    # Initialize the analysis system
    healthcare_system = HealthcareIoTAnalysis(sequence_length=30)

    print("Preprocessing data...")
    # Preprocess data
    X_train, X_test, y_train, y_test, feature_names = healthcare_system.preprocess_data(df, 'Target', feature_names=feature_names)

    # Build model
    input_shape = (X_train.shape[1], X_train.shape[2])
    num_classes = len(np.unique(y_train))
    healthcare_system.build_model(input_shape, num_classes)

    print("Training model...")
    # Train model
    healthcare_system.train(X_train, y_train, X_test, y_test)

    print("Evaluating model...")
    # Evaluate model (but don't show visualizations)
    metrics = healthcare_system.evaluate(X_test, y_test, show_plots=False)

    print(f"\nModel Performance:")
    print(f"Accuracy: {metrics['accuracy']:.4f}")
    print(f"Precision: {metrics['precision']:.4f}")
    print(f"Recall: {metrics['recall']:.4f}")
    print(f"F1 Score: {metrics['f1']:.4f}")

    # Now focus on custom patient assessment
    print("\n" + "="*50)
    print("CUSTOM PATIENT ASSESSMENT")
    print("="*50)

    # Example of a healthy patient
    print("\nAssessing a healthy patient...")
    healthy_patient = {
        'Temperature (C)': [36.5, 36.5, 36.6, 36.5, 36.6, 36.5, 36.6, 36.5, 36.6, 36.5, 36.6, 36.5, 36.6, 36.5, 36.6, 36.5, 36.6, 36.5, 36.6, 36.5, 36.6, 36.5, 36.6, 36.5, 36.6, 36.5, 36.6, 36.5, 36.6, 36.5],
        'Systolic_BP (mmHg)': [120, 121, 120, 119, 120, 121, 120, 119, 120, 121, 120, 119, 120, 121, 120, 119, 120, 121, 120, 119, 120, 121, 120, 119, 120, 121, 120, 119, 120, 121],
        'Diastolic_BP (mmHg)': [80, 80, 81, 80, 81, 80, 81, 80, 81, 80, 81, 80, 81, 80, 81, 80, 81, 80, 81, 80, 81, 80, 81, 80, 81, 80, 81, 80, 81, 80],
        'Heart_Rate (bpm)': [70, 71, 70, 72, 71, 70, 69, 70, 71, 72, 70, 69, 70, 71, 70, 69, 70, 71, 70, 69, 70, 71, 72, 71, 70, 69, 70, 71, 70, 69],
        'Device_Battery_Level (%)': [90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90],
        'Battery_Level (%)': [90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90]
    }

    healthcare_system.assess_custom_patient(healthy_patient, feature_names)

    # Example of a moderate risk patient
    print("\nAssessing a patient with moderate risk...")
    moderate_risk_patient = {
        'Temperature (C)': [36.8, 36.9, 37.0, 37.1, 37.2, 37.3, 37.4, 37.5, 37.6, 37.7, 37.8, 37.9, 38.0, 38.1, 38.2, 38.3, 38.4, 38.5, 38.6, 38.7, 38.8, 38.9, 39.0, 39.1, 39.2, 39.3, 39.4, 39.5, 39.6, 39.7],
        'Systolic_BP (mmHg)': [120, 122, 124, 126, 128, 130, 132, 134, 136, 138, 140, 142, 144, 146, 148, 150, 152, 154, 156, 158, 160, 162, 164, 166, 168, 170, 172, 174, 176, 178],
        'Diastolic_BP (mmHg)': [80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100, 102, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126, 128, 130, 132, 134, 136, 138],
        'Heart_Rate (bpm)': [75, 78, 80, 83, 85, 88, 90, 93, 95, 98, 100, 103, 105, 108, 110, 113, 115, 118, 120, 123, 125, 128, 130, 133, 135, 138, 140, 143, 145, 148],
        'Device_Battery_Level (%)': [85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85],
        'Battery_Level (%)': [85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85]
    }

    healthcare_system.assess_custom_patient(moderate_risk_patient, feature_names)

    # Example of a high-risk patient
    print("\nAssessing a high-risk patient...")
    high_risk_patient = {
        'Temperature (C)': [37.0, 37.2, 37.4, 37.6, 37.8, 38.0, 38.2, 38.4, 38.6, 38.8, 39.0, 39.2, 39.4, 39.6, 39.8, 40.0, 40.2, 40.4, 40.6, 40.8, 41.0, 41.2, 41.4, 41.6, 41.8, 42.0, 42.2, 42.4, 42.6, 42.8],
        'Systolic_BP (mmHg)': [130, 135, 140, 145, 150, 155, 160, 165, 170, 175, 180, 185, 190, 195, 200, 205, 210, 215, 220, 225, 230, 235, 240, 245, 250, 255, 260, 265, 270, 275],
        'Diastolic_BP (mmHg)': [90, 95, 100, 105, 110, 115, 120, 125, 130, 135, 140, 145, 150, 155, 160, 165, 170, 175, 180, 185, 190, 195, 200, 205, 210, 215, 220, 225, 230, 235],
        'Heart_Rate (bpm)': [80, 85, 90, 95, 100, 105, 110, 115, 120, 125, 130, 135, 140, 145, 150, 155, 160, 165, 170, 175, 180, 185, 190, 195, 200, 205, 210, 215, 220, 225],
        'Device_Battery_Level (%)': [80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80],
        'Battery_Level (%)': [80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80]
    }

    healthcare_system.assess_custom_patient(high_risk_patient, feature_names)

    # Allow user to input custom patient data
    print("\n" + "="*50)
    print("INTERACTIVE PATIENT ASSESSMENT")
    print("="*50)
    print("\nWould you like to assess your own patient data? (y/n)")
    response = input()

    if response.lower() == 'y':
        custom_patient = {}
        for feature in feature_names:
            print(f"\nEnter {feature} values (comma-separated list of 30 values):")
            values_input = input()
            try:
                values = [float(x.strip()) for x in values_input.split(',')]
                if len(values) != 30:
                    print(f"Warning: Expected 30 values, got {len(values)}. Using default values.")
                    values = [0] * 30
                custom_patient[feature] = values
            except:
                print("Invalid input. Using default values.")
                custom_patient[feature] = [0] * 30

        healthcare_system.assess_custom_patient(custom_patient, feature_names)

if __name__ == "__main__":
    main()